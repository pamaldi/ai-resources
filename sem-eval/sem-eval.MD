# Resources

Quan, X., Valentino, M., Dennis, L., and Freitas, A. (2024). Verification and refinement of natural language explanations through llm-symbolic theorem proving. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 
https://arxiv.org/pdf/2405.01379
Natural-language explanations are often used as a stand-in for judging how well multi-step, explanation-based NLI models reason. But checking whether those explanations are actually valid is hard, because it usually requires building and annotating special datasets via crowdsourcingâ€”slow, expensive, and sometimes logically inconsistent.

This paper tackles that problem by combining **Large Language Models (LLMs)** with **Theorem Provers (TPs)** to **verify and improve** NLI explanations. It introduces a neuro-symbolic framework called **Explanation-Refiner** that:

* uses an LLM to **generate explanatory sentences**, **formalize** them, and **propose inference strategies**;
* then uses a theorem prover to **formally validate** the logic of those explanations (giving guarantees);
* and produces **feedback** that helps the system **refine and correct** flawed explanations.

The authors show that Explanation-Refiner can be used both to **evaluate** LLMs (reasoning quality, auto-formalization, error correction) and to **automatically enhance** explanations of varying complexity across multiple domains.


Bertolazzi, L., Gatt, A., and Bernardi, R. (2024). A systematic analysis of large language models as soft reasoners: The case of syllogistic inferences. EMNLP 2024.
https://arxiv.org/pdf/2406.11341
The reasoning abilities of Large Language
Models (LLMs) are becoming a central focus
of study in NLP. In this paper, we consider the
case of syllogistic reasoning, an area of deductive reasoning studied extensively in logic and
cognitive psychology. Previous research has
shown that pre-trained LLMs exhibit reasoning
biases, such as content effects, avoid answering
that no conclusion follows, display human-like
difficulties, and struggle with multi-step reasoning. We contribute to this research line by
systematically investigating the effects of chainof-thought reasoning, in-context learning (ICL),
and supervised fine-tuning (SFT) on syllogistic reasoning, considering syllogisms with conclusions that support or violate world knowledge, as well as ones with multiple premises.
Crucially, we go beyond the standard focus on
accuracy, with an in-depth analysis of the conclusions generated by the models. Our results
suggest that the behavior of pre-trained LLMs
can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences,
although only the latter mitigates most reasoning biases without harming model consistency.1


A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences
https://arxiv.org/abs/2406.11341


conferenze
EMNLP
https://sites.google.com/view/nesynlp2025


tutorial
https://sites.google.com/view/nesynlp2025


persone
Leo Bertolazzi https://leobertolazzi.github.io/
Danilo Carvalho https://danilosc.com/ 
Freitas https://www.andrefreitas.net/

Neuro symbolic nlp https://drive.google.com/file/d/1bh6ghMD_7yU52rszT2ZBqHjK-52Yxil8/view

https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy?tab=readme-ov-file#-awesome-survey--books

